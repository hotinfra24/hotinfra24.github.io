<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>HotInfra 2024</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <!-- <link href="assets/img/favicon.png" rel="icon">
  <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon"> -->

  <!-- Google Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,700,700i|Raleway:300,400,500,700,800" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@100..900&family=Libre+Franklin:ital,wght@0,100..900;1,100..900&family=Zilla+Slab:ital,wght@0,300;0,400;0,500;0,600;0,700;1,300;1,400;1,500;1,600;1,700&display=swap" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">

  <!-- =======================================================
  * Template Name: TheEvent - v4.7.0
  * Template URL: https://bootstrapmade.com/theevent-conference-event-bootstrap-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NZW4W9RQFY"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-NZW4W9RQFY');
</script>

<body>

  <!-- ======= Header ======= -->
  <header id="header" class="d-flex align-items-center ">
    <div class="container-fluid container-xxl d-flex align-items-center">

      <div id="logo" class="me-auto">
        <!-- Uncomment below if you prefer to use a text logo -->
        <h1><a href="index.html">HotInfra 2024</a></h1>
        <!-- <a href="index.html" class="scrollto"><img src="assets/img/logo.png" alt="" title=""></a> -->
      </div>

      <nav id="navbar" class="navbar order-last order-lg-0">
        <ul>
          <li><a class="nav-link scrollto active" href="#hero">Home</a></li>
          <li><a class="nav-link scrollto" href="#about">About</a></li>
          <li><a class="nav-link scrollto" href="#speakers">Call for Papers</a></li>
          <li><a class="nav-link scrollto" href="#schedule">Program</a></li>
          <li><a class="nav-link scrollto" href="#organizers">Organizers</a></li>
          <li><a class="nav-link scrollto" href="#program-committee">Program Committee</a></li>
          <li><a class="nav-link scrollto" href="#past-events">Past Events</a></li>
          <!-- <li><a class="nav-link scrollto" href="#hotels">Hotels</a></li> -->
          <!-- <li><a class="nav-link" href="./gallery.html">Gallery</a></li> -->
          <!--<li><a class="nav-link scrollto" href="#supporters">Sponsors</a></li> -->
          <!-- <li class="dropdown"><a href="#"><span>Drop Down</span> <i class="bi bi-chevron-down"></i></a>
          <ul>
            <li><a href="#">Drop Down 1</a></li>
            <li class="dropdown"><a href="#"><span>Deep Drop Down</span> <i class="bi bi-chevron-right"></i></a>
              <ul>
                <li><a href="#">Deep Drop Down 1</a></li>
                <li><a href="#">Deep Drop Down 2</a></li>
                <li><a href="#">Deep Drop Down 3</a></li>
                <li><a href="#">Deep Drop Down 4</a></li>
                <li><a href="#">Deep Drop Down 5</a></li>
              </ul>
            </li>
            <li><a href="#">Drop Down 2</a></li>
            <li><a href="#">Drop Down 3</a></li>
            <li><a href="#">Drop Down 4</a></li>
          </ul>
        </li> -->
          <!-- <li><a class="nav-link scrollto" href="#footer">Contact</a></li> -->
        </ul>
        <i class="bi bi-list mobile-nav-toggle"></i>
      </nav><!-- .navbar -->
      <!-- <a class="buy-tickets scrollto" href="#buy-tickets">Buy Tickets</a> -->

    </div>
  </header><!-- End Header -->

  <!-- ======= Hero Section ======= -->
  <section id="hero">
    <div class="hero-container" data-aos="zoom-in" data-aos-delay="100">
      <h1 class="mb-4 pb-0">The 2nd Workshop on Hot Topics in <span>System Infrastructure</span></h1>
      <p class="mb-4 pb-0">November 3, 2024, Austin, Texas, USA 
      <br>Co-located with <a href="https://sigops.org/s/conferences/sosp/2024/">SOSP 2024</a></p>
      <!-- <a href="https://www.youtube.com/watch?v=jDDaplaOz7Q" class="glightbox play-btn mb-4"></a> -->
      <!-- <a href="#about" class="about-btn scrollto">More Details</a> -->
    </div>
  </section><!-- End Hero Section -->

  <main id="main">

    <!-- ======= About Section ======= -->
    <section id="about">
      <div class="container" data-aos="fade-up">
        <div class="row">
          <div class="col-lg-12">
            <h2>About</h2>
            <p>The 2nd Workshop on Hot Topics in System Infrastructure (HotInfra'24) provides a unique forum for
              cutting-edge research on system infrastructure and platforms. Researchers
              and engineers can share their recent research results and experiences, as well as discuss new
              challenges and opportunities in building next-generation system infrastructures, such as AI
              infrastructure, sustainable data centers, and edge/cloud computing infrastructure. The
              topics span across the full system stack with a focus on the design and implementation of
              system infrastructures. Relevant topics include hardware architecture, operating systems,
              runtime systems, and applications.</p>
          </div>
        </div>
      </div>
    </section><!-- End About Section -->

    <!-- ======= Speakers Section ======= -->
    <section id="speakers">
      <div class="container" data-aos="fade-up">
        <div class="section-header">
          <h2>Call for Papers</h2>
          <!-- <p>asdf asdf asdf</p> -->
        </div>

        <div class="row">
          <div class="col-lg-12">
              <!--<h2>Call for Papers</h2>-->
            <p>
              The HotInfra workshop is soliciting three types of paper submissions: <b>regular research papers</b>,
              <b>industry papers</b>, and <b>work-in-progress papers</b>: 
              <ol style="margin-top:-1.5em">
                  <li> 
                      The <b>regular research papers</b> may include studies that have been published
                  in top-tier systems and architecture conferences in the past year. We encourage
                  submissions that showcase new concepts and approaches for developing new and emerging
                  system infrastructures. 
                  </li>
                  <li>
                      The <b>industry papers</b> are encouraged to demonstrate the recent trends
                  and demands of real systems infrastructures from the industry and have insightful
                  discussions on the challenges and experiences of developing real system
                  infrastructures from industry perspectives. 
                  </li>
                  <li>
                      The <b>work-in-progress papers</b> are encouraged to have new and crazy ideas in building
                  future system infrastructure. We will favor submissions that have great potential to
                  inspire interesting discussions, so it is fine if the work has only an early version
                  of the system prototype.
                  </li>
              </ol>
            </p>
            <p>
              HotInfra'24 welcomes submissions on any topics related to system infrastructure and platforms.
              Specific areas include but are not limited to:
              <ul style="margin-top:-1.5em">
                <li>Systems architecture and hardware devices</li>
                <li>Operating systems and runtime systems support</li>
                <li>Resource management and task scheduling</li>
                <li>Empirical evaluation of real infrastructures</li>
                <li>Security and reliability of new infrastructures</li>
                <li>Energy efficiency and renewable energy supply</li>
                <li>Emerging applications and cloud services enabled</li>
                <li>Hardware virtualization</li>
                <li>Rack-scale infrastructure</li>
                <li>Software-defined data centers</li>
                <li>System-building approaches</li>
              </ul>
            </p>
            <h3>Submission Guidelines</h3>
            <p style="margin-bottom:0.5em">
              HotInfra'24 submissions must be no longer than three double-column pages excluding references. 
	      For regular research papers, we follow the single-blind review policy. For industry papers and work-in-progress 
	      papers, we follow the double-blind review policy. 
              All the accepted papers can be presented in the poster session by default. We will post
              presentation slides and accepted papers on the workshop website.
	      The authors can extend and publish their work in other conferences and
              journals after HotInfra. The HotInfra'24 workshop will also invite talks from industry and academia.
            </p>
            <p> <b>Please submit your work <a href="https://hotinfra24.hotcrp.com/">here</a>.</b>
            </p>
            <h3>Camera-Ready Guidelines</h3>
            <p style="margin-bottom:0.5em">
            <p>
              Please use <b><a href="assets/files/hotinfra24template.zip">the provided LaTeX template</a></b> to prepare your camera-ready paper.
              The camera-ready paper should be no longer than three double-column pages excluding references.
              Please submit your camera-ready paper via <b><a href="https://hotinfra24.hotcrp.com/">HotCRP</a></b> by clicking the "Edit submission" button and uploading your PDF file in the "Final version" field.
            </p>
          </div>


          <div class="col-lg-12">
            <h3>Important Dates</h3>
            <ul style="margin-top:0.5em">
              <li>
                <b>Submission Deadline:</b> <s style="color: rgba(0, 0, 0, 0.3);">September 1, 2024</s> September 8, 2024
              </li>
              <li>
                <b>Author Notifications:</b> September 30, 2024
              </li>
              <li>
                <b>Camera-ready Paper due:</b> October 14, 2024
              </li>
              <li>
                <b>Workshop:</b> November 3, 2024
              </li>
            </ul>
          </div>
        </div>
      </div>

    </section><!-- End Speakers Section -->

    <!-- ======= Schedule Section ======= -->
    <section id="schedule" class="section-with-bg">
      <div class="container" data-aos="fade-up">
        <div class="section-header">
          <h2>Workshop Program</h2>
          <p style="color: #0e1b4d; font-size: large;"><b>Location: Salon K, <a style="color: #0e1b4d;" href="https://www.hilton.com/en/hotels/auscvhh-hilton-austin/">Hilton Austin</a></b></p>
        </div>

        <!-- Remove "style=display:none" from div below to make it show -->
        <div class="tab-content row justify-content-center" data-aos="fade-up" data-aos-delay="200">
        

          <div role="tabpanel" class="col-lg-12 tab-pane fade show active" id="day-1">

            <!-- <div class="row schedule-item">
              <div class="col-md-10">
                <h3><b>Saturday, June 17th</b></h3>
              </div>
            </div> -->

            <div class="row schedule-item">
              <div class="col-md-2"><time>8:00 AM - 9:00 AM</time></div>
              <div class="col-md-10">
                <h4>Breakfast</h4>
              </div>
            </div>

            <div class="row schedule-item">
              <div class="col-md-2"><time>9:00 AM - 9:05 AM</time></div>
              <div class="col-md-10">
                <h4>Opening Remark</h4>
              </div>
            </div>

            <div class="row schedule-item">
              <div class="col-md-2"><time>9:05 AM - 10:05 AM</time></div>
              <div class="col-md-10" style="margin-bottom: 15px;">
                <h4>Keynote Talk</h4>
                <h4 class="keynote-title">Topic: To Be Announced</h4>
                <p class="speaker-name"><a href="https://web.stanford.edu/~kozyraki/">Christos Kozyrakis</a> (Stanford)</p>
                <br>
                <div class="speaker"><img src="assets/img/speakers/christos_kozyrakis.jpg" alt="Christos Kozyrakis"></div>
                <p>
                  <b>Abstract</b>
                  <br>
                  To be announced.
                </p>
                <!-- <hr>
                <p>
                  <b>Bio</b>
                  <br>
                </p> -->
              </div>
            </div>

            <div class="row schedule-item">
              <div class="col-md-2"><time>10:05 AM - 10:15 AM</time></div>
              <div class="col-md-10">
                <h4>Short Break</h4>
              </div>
            </div>

            <div class="row schedule-item">
              <div class="col-md-2"><time>10:15 AM - 10:45 AM</time></div>
              <div class="col-md-10">
                <h4>Session I: AI Platforms: Challenges and Opportunities</h4>
                <!-- <p class="session-chair">Session Chair: Name (Organization)</p> -->
                <ul>
                  <li>
                    <details>
                      <summary>
                        <div style="display: block; width: 100%;">
                          <div>
                            <div class="paper-title">Silent Data Corruptions in AI Systems: Evaluation and Mitigation</div>
                            <div class="paper-metadata">
                              <a href="./papers/hotinfra24-final83.pdf"><i title="Paper" class="bi bi-file-earmark-text"></i></a>
                            </div>
                          </div>
                          <div class="paper-authors">Xun Jiao, Fred Lin, Harish D. Dixit, Joel Coburn, Daniel Moore, Sriram Sankar (Meta)</div>
                        </div>
                      </summary>
                      <div class="abstract">
                        <p>
                          <b>Abstract:</b> The increasing complexity, heterogeneity, and scale of AI hardware systems make them increasingly susceptible to hardware faults, e.g., silent data corruption (SDC). Tackling this challenge requires answering the question: How to evaluate and mitigate the impact of SDCs on AI systems? For evaluation, we propose a novel quantitative metric, Parameter Vulnerability Factor (PVF), inspired by architectural vulnerability factor (AVF) in computer architecture, aiming to standardize the evaluation of SDC impact on AI models. PVF focuses on SDC occurring in model parameters &ndash; we define a model parameter's PVF as the probability that a corruption in that particular model parameter would result in an incorrect output. Through extensive fault injection (FI), we obtain PVF for a set of open-source models including DLRM, CNN, and BERT, as well as three Meta's production ranking and recommendation model, based on which we present unique insights. For mitigation, we propose Dr. DNA, a novel approach to detect and mitigate SDCs by formulating and extracting a set of unique SDC signatures from the Distribution of neuron activations (DNA). Through an extensive evaluation across 10 different models, results show that Dr. DNA achieves 100% SDC detection rate for most cases, 95% detection rate on average and >90% detection rate across all cases, representing 20% - 70% improvement over baselines. Dr. DNA also mitigates the impact of SDCs by recovering model performance with &lt;1% memory overhead and &lt;2.5% latency overhead.
                        </p>
                      </div>
                    </details>
                  </li>
                  <li>
                    <details>
                      <summary>
                        <div style="display: block; width: 100%;">
                          <div>
                            <div class="paper-title">Hardware-Assisted Virtualization of Neural Processing Units for Cloud Platforms</div>
                            <div class="paper-metadata">
                              <a href="./papers/hotinfra24-final82.pdf"><i title="Paper" class="bi bi-file-earmark-text"></i></a>
                            </div>
                          </div>
                          <div class="paper-authors">Yuqi Xue, Yiqi Liu (UIUC); Lifeng Nai (Google); Jian Huang (UIUC)</div>
                        </div>
                      </summary>
                      <div class="abstract">
                        <p>
                          <b>Abstract:</b> Cloud platforms today have been deploying hardware accelerators like neural processing units (NPUs) for powering machine learning (ML) inference services. To maximize the resource utilization while ensuring reasonable quality of service, a natural approach is to virtualize NPUs for efficient resource sharing for multi-tenant ML services. However, virtualizing NPUs for modern cloud platforms is not easy. This is not only due to the lack of system abstraction support for NPU hardware, but also due to the lack of architectural and ISA support for enabling fine-grained dynamic operator scheduling for virtualized NPUs.
                        </p>
                        <br>
                        <p>
                          We present Neu10, a holistic NPU virtualization framework. We investigate virtualization techniques for NPUs across the entire software and hardware stack. Neu10 consists of (1) a flexible NPU abstraction called vNPU, which enables fine-grained virtualization of the heterogeneous compute units in a physical NPU (pNPU); (2) a vNPU resource allocator that enables pay-as-you-go computing model and flexible vNPU-to-pNPU mappings for improved resource utilization and cost-effectiveness; (3) an ISA extension of modern NPU architecture for facilitating fine-grained tensor operator scheduling for multiple vNPUs. We implement Neu10 based on a production-level NPU simulator. Our experiments show that Neu10 improves the throughput of ML inference services by up to 1.4x and reduces the tail latency by up to 4.6x, while improving the NPU utilization by 1.2x on average, compared to state-of-the-art NPU sharing approaches.
                        </p>
                      </div>
                    </details>
                  </li>
                  <li>
                    <details>
                      <summary>
                        <div style="display: block; width: 100%;">
                          <div>
                            <div class="paper-title">Decluttering the Data Mess in LLM Training</div>
                            <div class="paper-metadata">
                              <a href="./papers/hotinfra24-final5.pdf"><i title="Paper" class="bi bi-file-earmark-text"></i></a>
                            </div>
                          </div>
                          <div class="paper-authors">Maximilian B&#246;ther, Dan Graur, Xiaozhe Yao, Ana Klimovic (ETH Zurich)</div>
                        </div>
                      </summary>
                      <div class="abstract">
                        <p>
                          <b>Abstract:</b> Training large language models (LLMs) presents new challenges for managing training data due to ever-growing model and dataset sizes. State-of-the-art LLMs are trained over trillions of tokens that are aggregated from a cornucopia of different datasets, forming collections such as RedPajama, Dolma, or FineWeb. However, as the data collections grow and cover more and more data sources, managing them becomes time-consuming, tedious, and prone to errors. The proportion of data with different characteristics (e.g., language, topic, source) has a huge impact on model performance.
                        </p>
                        <br>
                        <p>
                          In this work-in-progress paper, we discuss three challenges we observe for training LLMs due to the lack of system support for managing and mixing data collections. Then, we present Mixtera, a system to support LLM training data management.
                        </p>
                      </div>
                    </details>
                  </li>
                </ul>
              </div>
            </div>

            <div class="row schedule-item">
              <div class="col-md-2"><time>10:45 AM - 11:00 AM</time></div>
              <div class="col-md-10">
                <h4>Coffee Break</h4>
              </div>
            </div>

            <div class="row schedule-item">
              <div class="col-md-2"><time>11:00 AM - 12:00 PM</time></div>
              <div class="col-md-10">
                <h4>Session II: Cloud Efficiency: New Approaches New Opportunities</h4>
                <!-- <p class="session-chair">Session Chair: Name (Organization)</p> -->
                <ul>
                  <li>
                    <details>
                      <summary>
                        <div style="display: block; width: 100%;">
                          <div>
                            <div class="paper-title">SoK: Virtualization Challenges and Techniques in Serverless Computing</div>
                            <div class="paper-metadata">
                              <a href="./papers/hotinfra24-final86.pdf"><i title="Paper" class="bi bi-file-earmark-text"></i></a>
                            </div>
                          </div>
                          <div class="paper-authors">Vasudha Devarakonda, Aleksandr Earnest, Chia-Che Tsai (Texas A&M University)</div>
                        </div>
                      </summary>
                      <div class="abstract">
                        <p>
                          <b>Abstract:</b> This systematization of knowledge (SoK) paper summarizes the discussion of virtualization challenges and the corresponding techniques specific to serverless computing. We examine virtualization solutions, including paravirtualization, containers, lightweight hypervisors and kernels, and unikernels, and their applicability to serverless. Then, we discuss several challenges, including cold-start optimization, resource co-location, benchmarking, and the research-production gap, hoping to inspire future research.
                        </p>
                      </div>
                    </details>
                  </li>
                  <li>
                    <details>
                      <summary>
                        <div style="display: block; width: 100%;">
                          <div>
                            <div class="paper-title">Harmonizing Diverse Compute Resources for Efficiency</div>
                            <div class="paper-metadata">
                              <a href="./papers/hotinfra24-final34.pdf"><i title="Paper" class="bi bi-file-earmark-text"></i></a>
                            </div>
                          </div>
                          <div class="paper-authors">Dilina Dehigama, Shyam Jesalpura (University of Edinburgh); Marios Kogias (Imperial College London); Boris Grot (University of Edinburgh)</div>
                        </div>
                      </summary>
                      <div class="abstract">
                        <p>
                          <b>Abstract:</b> Online services are characterized by significant load fluctuations at find-grained intervals when coarse-grained load measurements indicate a relatively stable load. Running such services on virtual machines (VMs) rented from a cloud provider like AWS, which is a typical way to deploy online applications today, is inefficient due to the need to overprovision VM capacity to meet the SLO under variable load. In contrast, serverless computing is highly elastic but is prohibitively expensive for serving a large volume of requests. We thus argue for combining the different types of compute (i.e., VM and serverless instances) to achieve both cost-efficiency and elasticity. Our results show that hybrid compute is more cost effective than an optimal VM-only allocation that provisions just enough resource to meet the SLO using perfect knowledge of future load.
                        </p>
                      </div>
                    </details>
                  </li>
                  <li>
                    <details>
                      <summary>
                        <div style="display: block; width: 100%;">
                          <div>
                            <div class="paper-title">Designing Cloud Server for Lower Carbon</div>
                            <div class="paper-metadata">
                              <a href="./papers/hotinfra24-final22.pdf"><i title="Paper" class="bi bi-file-earmark-text"></i></a>
                            </div>
                          </div>
                          <div class="paper-authors">Jaylen Wang (Carnegie Mellon University); Daniel S. Berger (Microsoft; University of Washington); Fiodar Kazhamiaka, Celine Irvene, Chaojie Zhang, Esha Choukse, Kali Frost, Rodrigo Fonseca, Brijesh Warrier, Chetan Bansal, Jonathan Stern, Ricardo Bianchini (Microsoft); Akshitha Sriraman (Carnegie Mellon University)</div>
                        </div>
                      </summary>
                      <div class="abstract">
                        <p>
                          <b>Abstract:</b> To mitigate climate change, we must reduce carbon emissions from hyperscale cloud computing. We find that cloud compute servers cause the majority of emissions in a general-purpose cloud. Thus, we motivate designing carbon-efficient compute server SKUs, or GreenSKUs, using recently-available low-carbon server components. To this end, we design and build three GreenSKUs using low-carbon components, such as energy-efficient CPUs, reused old DRAM via CXL, and reused old SSDs.
                        </p>
                        <br>
                        <p>
                          We detail several challenges that limit GreenSKUs' carbon savings at scale and may prevent their adoption by cloud providers. To address these challenges, we develop a novel methodology and associated framework, GSF (Green SKU Framework), that enables a cloud provider to systematically evaluate a GreenSKU's carbon savings at scale. We implement GSF within Microsoft Azure's production constraints to evaluate our three GreenSKUs' carbon savings. Using GSF, we show that our most carbon-efficient GreenSKU reduces emissions per core by 28% compared to currently-deployed cloud servers. When designing GreenSKUs to meet applications' performance requirements, we reduce emissions by 15%. When incorporating overall data center overheads, our GreenSKU reduces Azure's net cloud emissions by 8%.
                        </p>
                      </div>
                    </details>
                  </li>
                  <li>
                    <details>
                      <summary>
                        <div style="display: block; width: 100%;">
                          <div>
                            <div class="paper-title">OpenInfra: A Co-simulation Framework for the Infrastructure Nexus</div>
                            <div class="paper-metadata">
                              <a href="./papers/hotinfra24-final1.pdf"><i title="Paper" class="bi bi-file-earmark-text"></i></a>
                            </div>
                          </div>
                          <div class="paper-authors">Jiaheng Lu, Yunming Xiao, Shmeelok Chakraborty (University of Michigan); Silvery Fu (UC Berkeley); Yoon Sung Ji, Ang Chen, Mosharaf Chowdhury (University of Michigan); Nalini Rao (EPRI); Sylvia Ratnasamy (UC Berkeley); Xinyu Wang (University of Michigan)</div>
                        </div>
                      </summary>
                      <div class="abstract">
                        <p>
                          <b>Abstract:</b> Critical infrastructures like datacenters, power grids, and water systems are interdependent, forming complex "infrastructure nexuses" that require co-optimization for efficiency, resilience, and sustainability. We present OpenInfra, a co-simulation framework designed to model these interdependencies by integrating domain-specific simulators for datacenters, power grids, and cooling systems but focusing on stitching them together for end-to-end experimentation. OpenInfra enables seamless integration of diverse simulators and flexible configuration of infrastructure interactions. Our evaluation demonstrates its ability to simulate large-scale infrastructure dynamics, including 7,392 servers over 100+ hours.
                        </p>
                      </div>
                    </details>
                  </li>
                </ul>
              </div>
            </div>

            <div class="row schedule-item">
              <div class="col-md-2"><time>12:00 PM - 1:00 PM</time></div>
              <div class="col-md-10">
                <h4>Lunch</h4>
              </div>
            </div>

            <div class="row schedule-item">
              <div class="col-md-2"><time>1:00 PM - 2:00 PM</time></div>
              <div class="col-md-10" style="margin-bottom: 15px;">
                <h4>Keynote Talk</h4>
                <h4 class="keynote-title">Topic: To Be Announced</h4>
                <p class="speaker-name"><a href="https://www.microsoft.com/en-us/research/people/ricardob/">Ricardo Bianchini</a> (Microsoft)</p>
                <br>
                <div class="speaker"><img src="assets/img/speakers/ricardo.jpg" alt="Ricardo Bianchini"></div>
                <p>
                  <b>Abstract</b>
                  <br>
                  To be announced.
                </p>
                <!-- <hr>
                <p>
                  <b>Bio</b>
                  <br>
                </p> -->
              </div>
            </div>

            <div class="row schedule-item">
              <div class="col-md-2"><time>2:00 PM - 2:15 PM</time></div>
              <div class="col-md-10">
                <h4>Short Break</h4>
              </div>
            </div>

            <div class="row schedule-item">
              <div class="col-md-2"><time>2:15 PM - 3:15 PM</time></div>
              <div class="col-md-10">
                <h4>Session III: AI Systems: Algorithms, Networking, and Hardware</h4>
                <!-- <p class="session-chair">Session Chair: Name (Organization)</p> -->
                <ul>
                  <li>
                    <details>
                      <summary>
                        <div style="display: block; width: 100%;">
                          <div>
                            <div class="paper-title">Immediate Communication for Distributed AI Tasks</div>
                            <div class="paper-metadata">
                              <a href="./papers/hotinfra24-final2.pdf"><i title="Paper" class="bi bi-file-earmark-text"></i></a>
                            </div>
                          </div>
                          <div class="paper-authors">Jihao Xin (KAUST); Seongjong Bae, KyoungSoo Park (Seoul National University); Marco Canini (KAUST); Changho Hwang (Microsoft Research)</div>
                        </div>
                      </summary>
                      <div class="abstract">
                        <p>
                          <b>Abstract:</b> Large AI models have necessitated efficient communication strategies across multi-GPU and multi-node infrastructures due to their increasing complexity. Current methods focusing on inter-operator overlaps fail when dependencies exist, leading to underutilized hardware. DistFuse addresses this by enabling fine-grained overlapping of computation and communication, triggering communication as soon as data is ready, and reducing latency. Initial experiments show up to 44.3% reduction in communication latency of Llama3-70B inference on a single node, demonstrating its potential to accelerate diverse AI workloads.
                        </p>
                      </div>
                    </details>
                  </li>
                  <li>
                    <details>
                      <summary>
                        <div style="display: block; width: 100%;">
                          <div>
                            <div class="paper-title">Do Large Language Models Need a Content Delivery Network?</div>
                            <div class="paper-metadata">
                              <a href="./papers/hotinfra24-final19.pdf"><i title="Paper" class="bi bi-file-earmark-text"></i></a>
                            </div>
                          </div>
                          <div class="paper-authors">Yihua Cheng, Kuntai Du, Jiayi Yao, Junchen Jiang (The University of Chicago)</div>
                        </div>
                      </summary>
                      <div class="abstract">
                        <p>
                          <b>Abstract:</b> As the use of large language models (LLMs) expands rapidly, so does the range of knowledge needed to supplement various LLM queries. Thus, enabling flexible and efficient injection of new knowledge in LLM inference is critical. Three high-level options exist: (i) embedding the knowledge in LLM's weights (i.e., fine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e., in-context learning), or (iii) injecting the KV caches of the new knowledge to LLM during prefill. This paper argues that, although fine-tuning and in-context learning are popular, using KV caches as the medium of knowledge could simultaneously enable more modular management of knowledge injection and more efficient LLM serving with low cost and fast response. To realize these benefits, we envision a Knowledge Delivery Network (KDN), a new system component in LLM services that dynamically optimizes the storage, transfer, and composition of KV cache across LLM engines and other compute and storage resources. We believe that, just like content delivery networks (CDNs), such as Akamai, enabled the success of the Internet ecosystem through their efficient data delivery, KDNs will be critical to the success of LLM applications through their efficient knowledge delivery. We have open-sourced a KDN prototype in <a href="https://github.com/LMCache/LMCache">https://github.com/LMCache/LMCache</a>.
                        </p>
                      </div>
                    </details>
                  </li>
                  <li>
                    <details>
                      <summary>
                        <div style="display: block; width: 100%;">
                          <div>
                            <div class="paper-title">Scaling Deep Learning Computation over the Inter-core Connected Intelligence Processor</div>
                            <div class="paper-metadata">
                              <a href="./papers/hotinfra24-final10.pdf"><i title="Paper" class="bi bi-file-earmark-text"></i></a>
                            </div>
                          </div>
                          <div class="paper-authors">Yiqi Liu, Yuqi Xue (UIUC); Yu Cheng (Peking University; Microsoft); Lingxiao Ma, Ziming Miao, Jilong Xue (Microsoft); Jian Huang (UIUC)</div>
                        </div>
                      </summary>
                      <div class="abstract">
                        <p>
                          <b>Abstract:</b> As AI chips incorporate numerous parallelized cores to scale deep learning (DL) computing, chips like Graphcore IPU enable high-bandwidth and low-latency inter-core links. They allow each core to directly access other cores' scratchpad memory, which enables new parallel computing paradigms. However, without proper support for the inter-core connections in current DL compilers, it is hard to exploit the benefits of this new architecture. We present T10, the first DL compiler to exploit the inter-core bandwidth and distributed on-chip memory on AI chips. To formulate the computation and communication patterns of tensor operators in the new architecture, T10 introduces a distributed tensor abstraction rTensor. T10 maps a DNN model to execution plans with a compute-shift pattern, by partitioning DNN computation into sub-operators and mapping them to cores, so that the cores can exchange data following predictable patterns. T10 alleviates unnecessary inter-core communications, makes globally optimized trade-offs between on-chip memory usage and inter-core communication overhead, and selects the best execution plan from a vast optimization space.
                        </p>
                      </div>
                    </details>
                  </li>
                  <li>
                    <details>
                      <summary>
                        <div style="display: block; width: 100%;">
                          <div>
                            <div class="paper-title">Optimizing Transformer Inference with Selective Distillation: Layerwise Conversion to Linear Attention</div>
                            <div class="paper-metadata">
                              <a href="./papers/hotinfra24-final56.pdf"><i title="Paper" class="bi bi-file-earmark-text"></i></a>
                            </div>
                          </div>
                          <div class="paper-authors">Yeonju Ro, Zhenyu Zhang, Vijay Chidambaram, Aditya Akella (UT Austin)</div>
                        </div>
                      </summary>
                      <div class="abstract">
                        <p>
                          <b>Abstract:</b> Transformer-like architectures with softmax attention have demonstrated exceptional performance in language modeling by capturing long-term token dependencies and ensuring efficient training. However, their scalability is constrained by the quadratic computation costs incurred during inference. To mitigate this issue, subquadratic models with SSMs and linear attentions have been introduced; however, such models achieve lower accuracy than Transformers. In this work, we aim to get the best of both worlds: we seek to convert carefully selected attention layers in a Transformer to gated linear attention layers, without sacrificing accuracy. Specifically, we analyze the performance benefits of subquadratic models and propose a distillation method that progressively converts the attention layer to a gated linear attention layer. While selecting layers to convert, we leverage downstream task accuracy as a criterion to minimize the accuracy loss caused by conversion. Our evaluation demonstrates that task accuracy is an effective criteria that can be used in adaptive distillation.
                        </p>
                      </div>
                    </details>
                  </li>
                </ul>
              </div>
            </div>

            <div class="row schedule-item">
              <div class="col-md-2"><time>3:15 PM - 3:45 PM</time></div>
              <div class="col-md-10">
                <h4>Coffee Break</h4>
              </div>
            </div>

            <div class="row schedule-item">
              <div class="col-md-2"><time>3:45 PM - 4:30 PM</time></div>
              <div class="col-md-10">
                <h4>Session IV: Resource Scheduling: Not Only for Efficiency But Also for Societal Good</h4>
                <!-- <p class="session-chair">Session Chair: Name (Organization)</p> -->
                <ul>
                  <li>
                    <details>
                      <summary>
                        <div style="display: block; width: 100%;">
                          <div>
                            <div class="paper-title">Adaptive Resource Allocation to Enhance the Kubernetes Performance for Large-Scale Clusters</div>
                            <div class="paper-metadata">
                              <a href="./papers/hotinfra24-final26.pdf"><i title="Paper" class="bi bi-file-earmark-text"></i></a>
                            </div>
                          </div>
                          <div class="paper-authors">Jiayin Luo, Xinkui Zhao, Yuxin Ma, Shengye Pang, Shuiguang Deng, Jianwei Yin (Zhejiang University)</div>
                        </div>
                      </summary>
                      <div class="abstract">
                        <p>
                          <b>Abstract:</b> The advent of cloud computing has led to a dramatic increase in the deployment of hyper-scale, diverse workloads in containerized form on cloud infrastructures. This expansion necessitates the management of numerous large-scale clusters. However, Kubernetes (k8s), the industry standard for container orchestration, faces challenges with low scheduling throughput and high request latency in such environments. We identify resource contention among components co-located on the same master node as a primary bottleneck. To address this, we introduce a lightweight framework designed to enhance the performance and scalability of Kubernetes clusters. Our approach adaptively allocates resources among co-located components, improving their overall performance. Implemented as a non-intrusive solution, our evaluations across various cluster scales show significant improvements, with a 7.3x increase in cluster scheduling throughput and a 37.3% reduction in request latency, surpassing the performance of vanilla Kubernetes and baseline resource allocation strategies.
                        </p>
                      </div>
                    </details>
                  </li>
                  <li>
                    <details>
                      <summary>
                        <div style="display: block; width: 100%;">
                          <div>
                            <div class="paper-title">Mewz: Lightweight Execution Environment for WebAssembly with High Isolation and Portability using Unikernels</div>
                            <div class="paper-metadata">
                              <a href="./papers/hotinfra24-final21.pdf"><i title="Paper" class="bi bi-file-earmark-text"></i></a>
                            </div>
                          </div>
                          <div class="paper-authors">Soichiro Ueda (Kyoto University); Ai Nozaki (University of Tokyo); Daisuke Kotani, Yasuo Okabe (Kyoto University)</div>
                        </div>
                      </summary>
                      <div class="abstract">
                        <p>
                          <b>Abstract:</b> Cloud computing requires isolation and portability for workloads. Virtual machines, for isolation, and containers, for portability, are widely used to achieve these requirements these days. However, using VMs and containers together entails two problems. First, VMs and containers have overheads that degrade performance. In addition, container images depend on host operating systems and architectures. To solve these problems, we propose a new system that distributes applications as WebAssembly (Wasm) and runs them as unikernels. Additionally, we propose a mechanism to convert a Wasm application into a unikernel image with the Wasm binary AoT-compiled to native code. We evaluated the performance of the system by running a simple HTTP server compiled into Wasm. The result showed that it ran Wasm applications with lower overhead than existing technologies.
                        </p>
                      </div>
                    </details>
                  </li>
                  <li>
                    <details>
                      <summary>
                        <div style="display: block; width: 100%;">
                          <div>
                            <div class="paper-title">Demographic Bias in Web Scheduling Systems</div>
                            <!-- <div class="paper-metadata">
                              <a href=""><i title="Paper" class="bi bi-file-earmark-text"></i></a>
                            </div> -->
                          </div>
                          <div class="paper-authors">Sara Mahdizadeh Shahri, Akshitha Sriraman (Carnegie Mellon University)</div>
                        </div>
                      </summary>
                      <div class="abstract">
                        <p>
                          <b>Abstract:</b> Modern web systems, especially scheduling systems, often adopt a "performance-first" approach, where they prioritize sending quick responses to the end user to improve Quality of Experience (QoE). We posit that putting performance first, however, might make scheduling systems introduce request priorities that may cause responses to be biased against certain user demographics. For example, to improve QoE, existing scheduling systems often prioritize requests that face lower network delays, which may implicitly cause biases against some requests that originate from rural areas.
                        </p>
                        <br>
                        <p>
                          To validate our hypothesis, we systematically study and define demographic bias for scheduling systems. We investigate whether existing scheduling systems can (unintentionally) introduce demographic bias to improve performance, precipitating discrimination against certain user demographics. We detail two case studies to show that demographic bias can occur in open-source schedulers. We then design BiasCheck, a scheduler that meets performance goals while reducing bias. We demonstrate that BiasCheck significantly reduces demographic bias while causing only a few SLO violations, compared to existing schedulers.
                        </p>
                      </div>
                    </details>
                  </li>
                </ul>
              </div>
            </div>

            <div class="row schedule-item">
              <div class="col-md-2"><time>4:30 PM - 4:45 PM</time></div>
              <div class="col-md-10">
                <h4>Short Break</h4>
              </div>
            </div>

            <div class="row schedule-item">
              <div class="col-md-2"><time>4:45 PM - 5:30 PM</time></div>
              <div class="col-md-10">
                <h4>Session V: Infrastructure Development: New Tools with New Techniques</h4>
                <!-- <p class="session-chair">Session Chair: Name (Organization)</p> -->
                <ul>
                  <li>
                    <details>
                      <summary>
                        <div style="display: block; width: 100%;">
                          <div>
                            <div class="paper-title">Revisiting Distributed Programming in the CXL Era</div>
                            <div class="paper-metadata">
                              <a href="./papers/hotinfra24-final74.pdf"><i title="Paper" class="bi bi-file-earmark-text"></i></a>
                            </div>
                          </div>
                          <div class="paper-authors">Teng Ma (Alibaba Group); Mingxing Zhang, Kang Chen, Jialiang Huang (Tsinghua University); Zheng Liu (Alibaba Group); Yongwei Wu (Tsinghua University)</div>
                        </div>
                      </summary>
                      <div class="abstract">
                        <p>
                          <b>Abstract:</b> As Moore's Law slows, distributed programming is increasingly prioritized to enhance system performance through horizontal scaling. This paper revisits the paradigms of message passing (MP) and distributed shared memory (DSM) regarding evolving interconnect technologies, particularly Compute Express Link (CXL). DSM's unified memory space offers simplicity in distributed programming by abstracting data communication complexities, but MP remains more prevalent due to its flexibility and programming-friendliness. We explore the memory sharing and pooling of CXL, which enable low-latency, coherent data transfers between multiple hosts. We address the complexities involved in managing shared states, particularly in the context of partial failures &ndash; termed Partial Failure Resilient DSM (RDSM). Thus we propose a memory management system named CXLSHM that leverages reference counting and distributed vector clocks for robust memory management. To be compatible with the message passing model, we introduce a CXL-based RPC named HydraRPC, which utilizes CXL shared memory to bypass data copying and serialization.
                        </p>
                      </div>
                    </details>
                  </li>
                  <li>
                    <details>
                      <summary>
                        <div style="display: block; width: 100%;">
                          <div>
                            <div class="paper-title">Towards Optimal Remote JIT Compilation Scheduling for the JVM</div>
                            <div class="paper-metadata">
                              <a href="./papers/hotinfra24-final79.pdf"><i title="Paper" class="bi bi-file-earmark-text"></i></a>
                            </div>
                          </div>
                          <div class="paper-authors">
                            Richard Kha, Nikhil Sreekumar (University of Toronto); Alexey Khrabrov (University of Toronto; IBM); Eyal de Lara, Angela Demke Brown, Moshe Gabel (University of Toronto); Marius Pirvu (IBM)
                          </div>
                        </div>
                      </summary>
                      <div class="abstract">
                        <p>
                          <b>Abstract:</b> In the Java Virtual Machine (JVM), Just-In-Time (JIT) compilation is used to speed up Java applications, however, JIT compilation incurs significant memory and CPU runtime overheads. JITServer is a disaggregated JIT compiler in the Eclipse OpenJ9 JVM &ndash; it decouples the JIT compiler from the JVM, thereby reducing CPU and memory requirements of JVM client applications.
                        </p>
                        <br>
                        <p>
                          JITServer schedules remote compilation requests from connected clients in first-come, first-served (FCFS) order. We show that when a JITServer instance is under high load, the scheduler performs better when it considers client information for each compilation request. Prioritizing requests from newly connected clients helps those clients warm-up rapidly, but risks starving requests from older clients. We developed a scheduling algorithm we call ALDCF (Alternating Least Done Client First) that prioritizes requests from clients with fewer completed compilation requests, while reducing starvation of older clients through alternating with FCFS. In our experiments, ALDCF reduces the average client JVM runtime by up to 9% compared to FCFS, while controlling starvation.
                        </p>
                      </div>
                    </details>
                  </li>
                  <li>
                    <details>
                      <summary>
                        <div style="display: block; width: 100%;">
                          <div>
                            <div class="paper-title">Lotus: Characterize Architecture Level CPU-based Preprocessing in Machine Learning Pipelines</div>
                            <div class="paper-metadata">
                              <a href="./papers/hotinfra24-final75.pdf"><i title="Paper" class="bi bi-file-earmark-text"></i></a>
                            </div>
                          </div>
                          <div class="paper-authors">Rajveer Bachkaniwala, Harshith Lanka, Kexin Rong, Ada Gavrilovska (Georgia Institute of Technology)</div>
                        </div>
                      </summary>
                      <div class="abstract">
                        <p>
                          <b>Abstract:</b> With the slowdown of Moore's law, the performance of data-intensive workloads require significant compute capacity and power. In machine learning (ML) pipelines, preprocessing has significant compute and power requirement, involving tasks such as loading, decoding, and applying transformations. To combat the preprocessing bottleneck, substantial efforts have been made with prior works introducing optimizations such as CPU and I/O parallelism, accelerator offloading, on-node/distributed computing as well as caching strategies. However, less attention has been focused on optimizing the preprocessing pipeline as a function of the CPU architecture's efficiency in executing it. The performance of preprocessing tasks is intrinsically linked to the CPU's microarchitecture, memory hierarchy, and the efficiency of its instruction pipeline.
                        </p>
                        <br>
                        <p>
                          Our work contributes to alleviate the gap by introducing LOTUS, a profiling tool specifically designed for the preprocessing stage of ML pipelines to aid future optimizations across the stack. The insights provided by LOTUS enable practitioners to evaluate the limitations of their CPU architecture and the efficiency of their preprocessing pipelines across different configurations, such as varying the number of preprocessing workers.
                        </p>
                      </div>
                    </details>
                  </li>
                </ul>
              </div>
            </div>

            <div class="row schedule-item">
              <div class="col-md-2"><time>5:30 PM</time></div>
              <div class="col-md-10">
                <h4>SOSP Conference Reception</h4>
              </div>
            </div>

            <div class="row schedule-item" style="padding-top: 3rem;">
              <div class="col-md-2"></div>
              <div class="col-md-10">
                <h3>List of Papers Accepted as Posters</h3>
                <!-- <p class="session-chair">Session Chair: Name (Organization)</p> -->
                <ul>
                  <li>
                    <details>
                      <summary>
                        <div style="display: block; width: 100%;">
                          <div>
                            <div class="paper-title">LLM Inference Performance on Chiplet-based Architectures and Systems</div>
                            <div class="paper-metadata">
                              <a href="./papers/hotinfra24-final97.pdf"><i title="Paper" class="bi bi-file-earmark-text"></i></a>
                            </div>
                          </div>
                          <div class="paper-authors">Surim Oh (UC Santa Cruz); Eric Qin, Yang Yang, Mengchi Zhang, Raj Parihar, Ashish Pandya (Meta)</div>
                        </div>
                      </summary>
                      <div class="abstract">
                        <p>
                          <b>Abstract:</b> Large Language Models (LLMs) have become increasingly prevalent, enabling a wide range of tasks across various platforms, from handheld devices and wearables to large-scale datacenters. Many of these applications, such as co-pilot and chatbot, rely on decoder-only style LLMs with multibillion parameters, which require significant computational resources to achieve desired performance metrics. As LLM workloads continue to evolve and demand more substantial computational resources, it is essential to explore innovative approaches to improve their performance.
                        </p>
                        <br>
                        <p>
                          One promising approach is the Multi-Chip-Module (MCM) architecture, which offers high-performance computing, storage, and network capabilities. However, the performance characteristics of LLMs on MCM architectures are not yet fully understood. To address this knowledge gap, we conducted a series of carefully designed experiments to investigate LLM inference performance on various MCM architectures. Our study provides detailed sensitivity analyses of dieto-die bandwidth, cache policies, and chiplet configurations, offering valuable insights into optimizing LLM performance on MCM architectures.
                        </p>
                      </div>
                    </details>
                  </li>
                  <li>
                    <details>
                      <summary>
                        <div style="display: block; width: 100%;">
                          <div>
                            <div class="paper-title">Towards Static Analysis of Interrupt Blocking Time in Operating System Kernels</div>
                            <div class="paper-metadata">
                              <!-- <a href="./papers/hotinfra24-xx.pdf"><i title="Paper" class="bi bi-file-earmark-text"></i></a> -->
                            </div>
                          </div>
                          <div class="paper-authors">Thomas Preisner, Dustin Nguyen, Jonathan Krebs, Rüdiger Kapitza, Wolfgang Schröder-Preikschat (Friedrich-Alexander-Universität Erlangen-Nürnberg)</div>
                        </div>
                      </summary>
                      <div class="abstract">
                        <p>
                          <b>Abstract:</b> Interrupts are integral to the interaction between an OS and its managed devices. They notify the OS that something requires immediate attention. However, they also disrupt ongoing executions by design and, therefore, have to be inhibited in some cases. This process requires utmost care as unnecessarily long interrupt blocking times negatively impact system performance. In the worst case, missing those may even result in catastrophic failures. The associated cause analysis for delayed interrupt reception is often time-consuming and costly due to the complex interaction between devices and the system. Especially on commodity server systems with many devices, the cause of problems related to interrupt latencies is often masked.
                        </p>
                        <br>
                        <p>
                          In this paper, we make an initial effort towards providing means to developers of general-purpose OSs to minimize the worst-case interrupt-delivery latency by identifying excessive interrupt blocking times. We provide insights for the system developer into potentially problematic code paths where interrupts are masked by leveraging symbolic execution of LLVM IR for general-purpose OSs. We currently work with x86_64, but the analysis is applicable to other architectures with minimal adaptions.
                        </p>
                      </div>
                    </details>
                  </li>
                  <li>
                    <details>
                      <summary>
                        <div style="display: block; width: 100%;">
                          <div>
                            <div class="paper-title">Evaluating Infrastructure as Code: Key Metrics and Performance Benchmarks</div>
                            <div class="paper-metadata">
                              <a href="./papers/hotinfra24-final8.pdf"><i title="Paper" class="bi bi-file-earmark-text"></i></a>
                            </div>
                          </div>
                          <div class="paper-authors">Aditya Gupta, Paras Mittal, Dr. Kunal Korgaonkar (BITS Pilani, K K Birla Goa Campus)</div>
                        </div>
                      </summary>
                      <div class="abstract">
                        <p>
                          <b>Abstract:</b> Organizations are increasingly adopting Infrastructure as Code (IaC) to automate the management and provisioning of cloud resources, moving away from manual configurations. Despite its growing usage, there is a lack of comprehensive research evaluating the performance of IaC tools in large-scale, real-world scenarios with complex architectures. This paper addresses this gap by comparing Terraform and AWS CloudFormation, two leading IaC tools, across key performance metrics such as CPU usage, memory consumption, system time, and user time. Using the TrainTicket project, which encompasses 47 microservices, we evaluate both tools in a controlled environment to provide insights into their effectiveness in managing large-scale cloud infrastructures. Our findings offer valuable guidance for organizations choosing between Terraform and CloudFormation in enterprise-scale deployments.
                        </p>
                      </div>
                    </details>
                  </li>
                  <li>
                    <details>
                      <summary>
                        <div style="display: block; width: 100%;">
                          <div>
                            <div class="paper-title">Thallus: An RDMA-based Columnar Data Transport Protocol</div>
                            <div class="paper-metadata">
                              <a href="./papers/hotinfra24-final12.pdf"><i title="Paper" class="bi bi-file-earmark-text"></i></a>
                            </div>
                          </div>
                          <div class="paper-authors">Jayjeet Chakraborty (UC Santa Cruz); Matthieu Dorier, Philip Carns, Robert Ross (Argonne National Laboratory); Carlos Maltzahn, Heiner Litz (UC Santa Cruz)</div>
                        </div>
                      </summary>
                      <div class="abstract">
                        <p>
                          <b>Abstract:</b> The volume of data generated and stored in contemporary global data centers is experiencing exponential growth. This rapid data growth necessitates efficient processing and analysis to extract valuable business insights. In distributed data processing systems, data undergoes exchanges between the compute servers that contribute significantly to the total data processing duration in adequately large clusters, necessitating efficient data transport protocols.
                        </p>
                        <br>
                        <p>
                          Traditionally, data transport frameworks such as JDBC and ODBC have used TCP/IP-over-Ethernet as their underlying network protocol. Such frameworks require serializing the data into a single contiguous buffer before handing it off to the network card, primarily due to the requirement of contiguous data in TCP/IP. In OLAP use cases, this serialization process is costly for columnar data batches as it involves numerous memory copies that hurt data transport duration and overall data processing performance. We study the serialization overhead in the context of a widely-used columnar data format, Apache Arrow, and propose leveraging RDMA to transport Arrow data over Infiniband in a zero-copy manner. We design and implement Thallus, an RDMA-based columnar data transport protocol for Apache Arrow based on the Thallium framework from the Mochi ecosystem, compare it with a purely Thallium RPC-based implementation, and show substantial performance improvements can be achieved by using RDMA for columnar data transport.
                        </p>
                      </div>
                    </details>
                  </li>
                  <li>
                    <details>
                      <summary>
                        <div style="display: block; width: 100%;">
                          <div>
                            <div class="paper-title">Delayed Privilege Escalation for Fast System Calls</div>
                            <div class="paper-metadata">
                              <!-- <a href="./papers/hotinfra24-xx.pdf"><i title="Paper" class="bi bi-file-earmark-text"></i></a> -->
                            </div>
                          </div>
                          <div class="paper-authors">Maximilian Ott, Phillip Raffeck, Rüdiger Kapitza, Wolfgang Schröder-Preikschat (Friedrich-Alexander-Universität Erlangen-Nürnberg)</div>
                        </div>
                      </summary>
                      <div class="abstract">
                        <p>
                          <b>Abstract:</b> Executing system calls requires switching the privilege level when entering the kernel. For high-performance applications, these costly switches can be omitted by directly linking with the kernel in the form of library operating systems or unikernels. However, this approach comes at the cost of compromising security as it removes user-/kernel-space separation. In other contexts, on-demand rather than proactive action allows paying overhead costs only when necessary.
                        </p>
                        <br>
                        <p>
                          We propose that single-address systems allow for an on-demand approach for system calls. When the operating-system data is hidden by the sheer size of the address space, elevated privileges are only necessary when performing privileged hardware operations. We believe that single-address systems would benefit from such an on-demand approach.
                        </p>
                        <br>
                        <p>
                          We introduce a concept that allows us to defer the privilege-level switch until it is needed, that is when the first privileged instruction is encountered. We present a prototype implementation of the concept as an adaption of the Linux kernel.
                        </p>
                      </div>
                    </details>
                  </li>
                  <li>
                    <details>
                      <summary>
                        <div style="display: block; width: 100%;">
                          <div>
                            <div class="paper-title">Industrial Trending on AI System Reliability: A Brief Review</div>
                            <div class="paper-metadata">
                              <!-- <a href="./papers/hotinfra24-xx.pdf"><i title="Paper" class="bi bi-file-earmark-text"></i></a> -->
                            </div>
                          </div>
                          <div class="paper-authors">Xun Jiao, Abhinav Pandey, Fred Lin (Meta)</div>
                        </div>
                      </summary>
                      <div class="abstract">
                        <p>
                          <b>Abstract:</b> As AI systems grow in complexity and scale, particularly with the training of large language models (LLMs) that require large-scale GPU clusters, ensuring their reliability becomes increasingly critical. In the large-scale training job, a single GPU failure can disrupt an entire training job, affecting thousands of interconnected GPUs. We present a brief literature review of the latest industrial trending and efforts, focusing on reliability and fault tolerance of large-scale AI infrastructure, from companies such as Meta, Microsoft, ByteDance, and Alibaba. The review is structured around two main themes: (1) hardware failure analysis and top root causes, and (2) fault tolerance with hardware validation, failure detection, and checkpointing-based recovery.
                        </p>
                      </div>
                    </details>
                  </li>
                  <li>
                    <details>
                      <summary>
                        <div style="display: block; width: 100%;">
                          <div>
                            <div class="paper-title">Towards Using Partitioned GPU Virtual Functions for Mixture of Experts</div>
                            <div class="paper-metadata">
                              <!-- <a href="./papers/hotinfra24-xx.pdf"><i title="Paper" class="bi bi-file-earmark-text"></i></a> -->
                            </div>
                          </div>
                          <div class="paper-authors">Tony Yi, Vignesh Chander, Vamsi Alla, Jerry Jiang (AMD)</div>
                        </div>
                      </summary>
                      <div class="abstract">
                        <p>
                          <b>Abstract:</b> Recent advancements in large language models (LLMs) have shown that smaller, fine-tuned models have comparable or better performance against larger general-purpose models in domain-specific knowledge, even when quantized. However, these models suffer from several issues in production systems: under-utilizing memory and potential data security risks. We propose a new method of mixture of experts (MoE) inference utilizing GPU partitioning combined with single-root IO virtualization (SRIOV), enabling better utilization of GPU memory and scalability, while ensuring model weights remain secure. LLMs today come in a variety of sizes and quantization levels, each with its own memory requirement. Using SRIOV, we can partition the GPU into one or more virtual functions (VFs), altering allocated memory and compute to fit the needs of these LLMs. With AMD Instinct™ MI300X, for example, one VF can have 24 to 192GB of high bandwidth memory (HBM), scaling into 1.5TB per node. These SRIOV-enabled virtual machines also address the load imbalance inherent in MoE models, eliminating the need for an auxiliary loss for load balancing, while maintaining fast interconnect between all components, providing low latency during inference. Additionally, isolation capabilities built into SRIOV ensure native data security as virtual functions are isolated from each other, creating the possibility of new use cases where different vendors may provide their own expert to the mixture.
                        </p>
                      </div>
                    </details>
                  </li>
                  <li>
                    <details>
                      <summary>
                        <div style="display: block; width: 100%;">
                          <div>
                            <div class="paper-title">Redesigning Edge-Centric Micro-Datacenters for Efficient Multitenancy</div>
                            <div class="paper-metadata">
                              <!-- <a href="./papers/hotinfra24-xx.pdf"><i title="Paper" class="bi bi-file-earmark-text"></i></a> -->
                            </div>
                          </div>
                          <div class="paper-authors">Sudarsun Kannan (Rutgers University); Ram Durairajan, River Bartz (University of Oregon); Uli Kremer (Rutgers University)</div>
                        </div>
                      </summary>
                      <div class="abstract">
                        <p>
                          <b>Abstract:</b> Hazard monitoring systems depend on micro datacenters (MicroDCs) in remote areas for early disaster detection and response, and continuous environmental monitoring. These MicroDCs, limited by resources and energy, require efficient multi-tenant resource management, as traditional datacenter solutions for isolation and resource partitioning are insufficient in their dynamic environments. This paper focuses on memory resource sharing across multitenant applications in MicroDCs, demonstrating through experimental analysis of a real-time object detection framework and a NoSQL database that static partitioning leads to suboptimal performance and energy consumption. Our results reveal a non-linear relationship between performance, energy consumption and memory allocation, highlighting the potential for significant energy savings through optimized resource management.
                        </p>
                      </div>
                    </details>
                  </li>
                </ul>
              </div>
            </div>

          </div>

        </div>

      </div>

    </section><!-- End Schedule Section -->

    <!-- ======= Organizer Section ======= -->
    <section id="organizers">

      <div class="container-fluid" data-aos="fade-up">

        <div class="section-header">
          <h2>Organizers</h2>
        </div>

        <div class="tab-content row justify-content-center" data-aos="fade-up" data-aos-delay="200">

          <div role="tabpanel" class="col-lg-9 tab-pane fade show active" id="day-1">

            <div class="row schedule-item">
              <div class="col-md-10">
                <h3><b>Organizing Committee</b></h3>
              </div>
            </div>

            <div class="row schedule-item">
              <div class="col-md-6">
                <div class="speaker">
                  <img src="assets/img/speakers/jianh.png" alt="Jian Huang">
                </div>
                <h4 class="organizer-font"><a href="http://jianh.web.engr.illinois.edu/">Jian Huang</a></h4>
                <h4 class="organizer-font"><span>University of Illinois Urbana-Champaign</span></h4>
                <p><a href="mailto:jianh@illinois.edu">jianh@illinois.edu</a></p>
              </div>
              <div class="col-md-6">
                <div class="speaker">
                  <img src="assets/img/speakers/cqtang.jpg" alt="Chunqiang Tang">
                </div>
                <h4 class="organizer-font"><a href="https://sites.google.com/site/tangchq">Chunqiang Tang</a></h4>
                <h4 class="organizer-font"><span>Meta</span></h4>
                <p><a href="mailto:tang@fb.com">tang@fb.com</a></p>
              </div>
            </div>

            <div class="row schedule-item">
              <div class="col-md-6">
                <div class="speaker">
                  <img src="assets/img/speakers/dnellans.jpg" alt="David Nellans">
                </div>
                <h4 class="organizer-font"><a href="https://research.nvidia.com/person/david-nellans">David Nellans</a></h4>
                <h4 class="organizer-font"><span>NVIDIA</span></h4>
                <p><a href="mailto:dnellans@nvidia.com">dnellans@nvidia.com</a></p>
              </div>
              <div class="col-md-6">
                <div class="speaker">
                  <img src="assets/img/speakers/jichuan.jfif" alt="Jichuan Chang">
                </div>
                <h4 class="organizer-font"><a href="https://research.google/people/JichuanChang/">Jichuan Chang</a></h4>
                <h4 class="organizer-font"><span>Google</span></h4>
                <p><a href="mailto:jichuan@google.com">jichuan@google.com</a></p>
              </div>
            </div>

          </div>

          <div role="tabpanel" class="col-lg-9 tab-pane fade show active" id="day-1">

            <div class="row schedule-item">
              <div class="col-md-10">
                <h3><b>Web Chair</b></h3>
              </div>
            </div>

            <div class="row schedule-item">
              <div class="col-md-6">
                <div class="speaker">
                  <img src="assets/img/speakers/yuqixue2.png" alt="Yuqi Xue">
                </div>
                <h4 class="organizer-font"><a href="https://xzman.github.io/yuqixue/">Yuqi Xue</a></h4>
                <h4 class="organizer-font"><span>University of Illinois Urbana-Champaign</span></h4>
                <p><a href="mailto:yuqixue2@illinois.edu">yuqixue2@illinois.edu</a></p>
              </div>
            </div>

          </div>

        </div>
      </div>

    </section><!-- End Organizers Section -->


    <!-- ======= Program Committee Section ======= -->
    <section id="program-committee">

      <div class="container-fluid" data-aos="fade-up">

        <div class="section-header">
          <h2>Program Committee</h2>
        </div>

        <div class="tab-content row justify-content-center" data-aos="fade-up" data-aos-delay="200">

          <div role="tabpanel" class="col-lg-9 tab-pane fade show active" id="day-1">
            <div class="row schedule-item">
              <div class="col-md-6">
                <div class="speaker">
                  <img src="assets/img/speakers/noman.jpg" alt="Noman Bashir">
                </div>
                <br>
                <h4 class="organizer-font"><a href="https://noman-bashir.github.io/">Noman Bashir</a></h4>
                <h4 class="organizer-font"><span>MIT</span></h4>
              </div>
              <div class="col-md-6">
                <div class="speaker">
                  <img src="assets/img/speakers/haggai.jpg" alt="Haggai Eran">
                </div>
                <br>
                <h4 class="organizer-font"><a href="https://haggaie.github.io/">Haggai Eran</a></h4>
                <h4 class="organizer-font"><span>NVIDIA</span></h4>
              </div>
            </div>

            <div class="row schedule-item">
              <div class="col-md-6">
                <div class="speaker">
                  <img src="assets/img/speakers/christina.jpg" alt="Christina Giannoula">
                </div>
                <br>
                <h4 class="organizer-font"><a href="https://cgiannoula.github.io/">Christina Giannoula</a></h4>
                <h4 class="organizer-font"><span>University of Toronto</span></h4>
              </div>
              <div class="col-md-6">
                <div class="speaker">
                  <img src="assets/img/speakers/Inigo-Goiri.jpg" alt="Inigo Goiri">
                </div>
                <br>
                <h4 class="organizer-font"><a href="https://www.microsoft.com/en-us/research/people/inigog/">Inigo Goiri</a></h4>
                <h4 class="organizer-font"><span>Microsoft</span></h4>
              </div>
            </div>

            <div class="row schedule-item">
              <div class="col-md-6">
                <div class="speaker">
                  <img src="assets/img/speakers/sagar.jpg" alt="Sagar Karandikar">
                </div>
                <br>
                <h4 class="organizer-font"><a href="https://sagark.org/">Sagar Karandikar</a></h4>
                <h4 class="organizer-font"><span>UC Berkeley; Google</span></h4>
              </div>
              <div class="col-md-6">
                <div class="speaker">
                  <img src="assets/img/speakers/fan.jpg" alt="Fan Lai">
                </div>
                <br>
                <h4 class="organizer-font"><a href="https://www.fanlai.me/">Fan Lai</a></h4>
                <h4 class="organizer-font"><span>UIUC; Google</span></h4>
              </div>
            </div>

            <div class="row schedule-item">
              <div class="col-md-6">
                <div class="speaker">
                  <img src="assets/img/speakers/mgliu.jpg" alt="Ming Liu">
                </div>
                <br>
                <h4 class="organizer-font"><a href="https://pages.cs.wisc.edu/~mgliu/">Ming Liu</a></h4>
                <h4 class="organizer-font"><span>UW-Madison</span></h4>
              </div>
              <div class="col-md-6">
                <div class="speaker">
                  <img src="assets/img/speakers/apoorve.jpg" alt="Apoorve Mohan">
                </div>
                <br>
                <h4 class="organizer-font"><a href="https://www.apoorve.com/">Apoorve Mohan</a></h4>
                <h4 class="organizer-font"><span>IBM Research</span></h4>
              </div>
            </div>

            <div class="row schedule-item">
              <div class="col-md-6">
                <div class="speaker">
                  <img src="assets/img/speakers/skarlatos.jpeg" alt="Dimitrios Skarlatos">
                </div>
                <br>
                <h4 class="organizer-font"><a href="https://www.cs.cmu.edu/~dskarlat/">Dimitrios Skarlatos</a></h4>
                <h4 class="organizer-font"><span>Carnegie Mellon University</span></h4>
              </div>
              <div class="col-md-6">
                <div class="speaker">
                  <img src="assets/img/speakers/yawenwang.jpg" alt="Yawen Wang">
                </div>
                <br>
                <h4 class="organizer-font"><a href="https://www.linkedin.com/in/yawen-wang-589b38113/">Yawen Wang</a></h4>
                <h4 class="organizer-font"><span>Google</span></h4>
              </div>
            </div>

            <div class="row schedule-item">
              <div class="col-md-6">
                <div class="speaker">
                  <img src="assets/img/speakers/ywang.jpg" alt="Yang Wang">
                </div>
                <br>
                <h4 class="organizer-font"><a href="https://yangwang83.github.io/">Yang Wang</a></h4>
                <h4 class="organizer-font"><span>The Ohio State University; Meta</span></h4>
              </div>
              <div class="col-md-6">
                <div class="speaker">
                  <img src="assets/img/speakers/jilong.jpg" alt="Jilong Xue">
                </div>
                <br>
                <h4 class="organizer-font"><a href="https://www.microsoft.com/en-us/research/people/jxue/">Jilong Xue</a></h4>
                <h4 class="organizer-font"><span>Microsoft</span></h4>
              </div>
            </div>

            <div class="row schedule-item">
              <div class="col-md-6">
                <div class="speaker">
                  <img src="assets/img/speakers/neeraja.jpg" alt="Neeraja Yadwadkar">
                </div>
                <br>
                <h4 class="organizer-font"><a href="https://sites.utexas.edu/neeraja/">Neeraja Yadwadkar</a></h4>
                <h4 class="organizer-font"><span>UT Austin</span></h4>
              </div>
              <div class="col-md-6">
                <div class="speaker">
                  <img src="assets/img/speakers/zi_yan.jpg" alt="Zi Yan">
                </div>
                <br>
                <h4 class="organizer-font"><a href="https://normal.zone/">Zi Yan</a></h4>
                <h4 class="organizer-font"><span>NVIDIA</span></h4>
              </div>
            </div>

            <div class="row schedule-item">
              <div class="col-md-6">
                <div class="speaker">
                  <img src="assets/img/speakers/pantea.jpg" alt="Pantea Zardoshti">
                </div>
                <br>
                <h4 class="organizer-font"><a href="https://www.microsoft.com/en-us/research/people/pzardoshti/">Pantea Zardoshti</a></h4>
                <h4 class="organizer-font"><span>Microsoft</span></h4>
              </div>
            </div>

          </div>

        </div>

      </div>

    </section><!-- End Program Committee Section -->


    <!-- ======= Past Events Section ======= -->
    <section id="past-events">

      <div class="container-fluid" data-aos="fade-up">

        <div class="tab-content row justify-content-center" data-aos="fade-up" data-aos-delay="200">

          <div role="tabpanel" class="col-lg-9 tab-pane fade show active" id="day-1">

            <div class="section-header">
              <h2>Past Events</h2>
            </div>

            <div class="row schedule-item">
            </div>

            <div class="row schedule-item">
              <div class="col">
                <h4 class="past-event-name">
                  <a href="https://hotinfra23.github.io/">1st Workshop on Hot Topics in System Infrastructure</a>
                  <span>(Co-located with ISCA 2023)</span>
                </h4>
              </div>
              <div class="col">
                <h4 style="text-align: right;">June 18, 2023, Orlando, Florida, USA</h4>
              </div>
            </div>
          
          </div>
        </div>
      </div>

    </section><!-- End Past Events Section -->


  </main><!-- End #main -->

  <!-- ======= Footer ======= -->
  <footer id="footer">
    <!-- <div class="footer-top">
      <div class="container">
        <div class="row">

          <div class="col-lg-3 col-md-6 footer-info"> -->
            <!-- <img src="assets/img/logo.png" alt="TheEvenet"> -->
            <!-- <h1><a href="https://platformxlab.github.io/">PlatformX</a></h1>
            <p>In alias aperiam. Placeat tempore facere. Officiis voluptate ipsam vel eveniet est dolor et totam porro. Perspiciatis ad omnis fugit molestiae recusandae possimus. Aut consectetur id quis. In inventore consequatur ad voluptate cupiditate debitis accusamus repellat cumque.</p>
          </div>

          <div class="col-lg-3 col-md-6 footer-links">
            <h4>Useful Links</h4>
            <ul>
              <li><i class="bi bi-chevron-right"></i> <a href="#">Home</a></li>
              <li><i class="bi bi-chevron-right"></i> <a href="#">About us</a></li>
              <li><i class="bi bi-chevron-right"></i> <a href="#">Services</a></li>
              <li><i class="bi bi-chevron-right"></i> <a href="#">Terms of service</a></li>
              <li><i class="bi bi-chevron-right"></i> <a href="#">Privacy policy</a></li>
            </ul>
          </div>

          <div class="col-lg-3 col-md-6 footer-links">
            <h4>Useful Links</h4>
            <ul>
              <li><i class="bi bi-chevron-right"></i> <a href="#">Home</a></li>
              <li><i class="bi bi-chevron-right"></i> <a href="#">About us</a></li>
              <li><i class="bi bi-chevron-right"></i> <a href="#">Services</a></li>
              <li><i class="bi bi-chevron-right"></i> <a href="#">Terms of service</a></li>
              <li><i class="bi bi-chevron-right"></i> <a href="#">Privacy policy</a></li>
            </ul>
          </div>

          <div class="col-lg-3 col-md-6 footer-contact">
            <h4>Contact Us</h4>
            <p>
              A108 Adam Street <br>
              New York, NY 535022<br>
              United States <br>
              <strong>Phone:</strong> +1 5589 55488 55<br>
              <strong>Email:</strong> info@example.com<br>
            </p>

            <div class="social-links">
              <a href="#" class="twitter"><i class="bi bi-twitter"></i></a>
              <a href="#" class="facebook"><i class="bi bi-facebook"></i></a>
              <a href="#" class="instagram"><i class="bi bi-instagram"></i></a>
              <a href="#" class="google-plus"><i class="bi bi-instagram"></i></a>
              <a href="#" class="linkedin"><i class="bi bi-linkedin"></i></a>
            </div>

          </div>

        </div>
      </div>
    </div> -->

    <div class="container">
      <div class="copyright">
        <!-- &copy; Copyright <strong>PlatformX</strong>. All Rights Reserved -->
      </div>
      <div class="credits">
        <!--
        All the links in the footer should remain intact.
        You can delete the links only if you purchased the pro version.
        Licensing information: https://bootstrapmade.com/license/
        Purchase the pro version with working PHP/AJAX contact form: https://bootstrapmade.com/buy/?theme=TheEvent
      -->
        Designed by <a href="https://bootstrapmade.com/">BootstrapMade</a>
      </div>
    </div>
  </footer><!-- End  Footer -->

  <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/aos/aos.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>
